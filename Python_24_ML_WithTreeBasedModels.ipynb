{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACHINE LEARNING WITH TREE-BASED MODELS IN PYTHON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision-Tree for Classication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classication-tree:\n",
    "\n",
    "#Sequence ofif-else questions about individual features.\n",
    "#Objective: infer class labels.\n",
    "#Able to capture non-linear relationships between features and labels.\n",
    "#Don't require feature scaling (ex: Standardization, ..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('C:/Users/stayde/Documents/Python Scripts/Python_24_MLwithTreeBasedModels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC = pd.read_csv('BreastCancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC.diagnosis.unique()\n",
    "\n",
    "#M - Maleficent\n",
    "#B - Benign\n",
    "\n",
    "X = BC.drop(['diagnosis', 'id'], axis = 1)\n",
    "\n",
    "y = BC['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classication-tree in scikit-learn:\n",
    "\n",
    "#Import Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Import train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import accuracy_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split dataset into 80%\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y ,test_size = 0.2, stratify = y, random_state = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dt:\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth = 2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035087719298246"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification-tree in scikit-learn:\n",
    "\n",
    "# Fit dt to the training set:\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels:\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate test-set accuracy:\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stayde\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets use logistics regression and compare the boundaries:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Regions:\n",
    "\n",
    "#Decision region: region in the feature space where all instances are assigned to one class label.\n",
    "#Decision Boundary: surface separating different decision regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification-Tree Learning:\n",
    "\n",
    "# Building Blocks of a Decision-Tree:\n",
    "\n",
    "# Decision-Tree: data structure consisting of a hierarchy of nodes.\n",
    "# Node: question or prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Blocks of a Decision-Tree:\n",
    "# Three kinds of nodes:\n",
    "\n",
    "# Root: no parent node, question giving rise to two children nodes.\n",
    "# Internal node: one parent node, question giving rise to two children nodes.\n",
    "# Leaf: one parent node, no children nodes --> prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criteria to measure the impurity of a node I(node):\n",
    "# gini index,\n",
    "# entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classication-Tree Learning:\n",
    "\n",
    "#Nodes are grown recursively.\n",
    "#At each node, split the data based on:\n",
    "    # feature f and split-point sp to maximize IG(node).\n",
    "    # If IG(node)= 0, declare the node a leaf....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dt, set criterion to gini:\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion = 'gini', random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=1, splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information Criterion in scikit-learn:\n",
    "\n",
    "# Fit dt to the training set:\n",
    "\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test-set labels:\n",
    "\n",
    "y_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298245614035088"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate test-set accuracy:\n",
    "\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision-Tree for Regression:\n",
    "\n",
    "#Lets consider auto mpg dataset:\n",
    "\n",
    "auto_old = pd.read_csv('auto.csv')\n",
    "\n",
    "auto = pd.get_dummies(auto_old, drop_first = True)\n",
    "\n",
    "X = auto.drop('mpg', axis =1)\n",
    "\n",
    "y = auto['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision-Tree for Regression:\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.2, random_state=3)\n",
    "\n",
    "# Instantiate a DecisionTreeRegressor 'dt':\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=4,min_samples_leaf=0.1, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.304002299698696\n"
     ]
    }
   ],
   "source": [
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "mse_dt = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "print(rmse_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalization Error:\n",
    "\n",
    "# Supervised Learning - Under the Hood\n",
    "    #Supervised Learning: y = f(x), f is unknown.\n",
    "\n",
    "# Goals of Supervised Learning:\n",
    "    \n",
    "# Find a model f' that best approximates f: f' ≈ f\n",
    "# f' can be LogisticRegression, Decision Tree, Neural Network ...\n",
    "# Discard noise as much as possible.\n",
    "# End goal: f' should acheive a low predictive error on unseen datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difculties in Approximating f:\n",
    "    \n",
    "# Overfitting: f'(x) fits the training set noise.\n",
    "# Underfitting: f' is not flexible enough to approximate f.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalization Error:\n",
    "\n",
    "# Generalization Error of f': Does f' generalize well on unseen data?\n",
    "# It can be decomposed as follows: Generalization Error of \n",
    "\n",
    "#  f' = bias^2 + variance + irreducible error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bias:\n",
    "    \n",
    "#Bias error term that tells you, on average, how much f' ≠ f.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variance:\n",
    "    \n",
    "#Variance:tells you how much f' is inconsistent over different training sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Complexity:\n",
    "    \n",
    "#Model Complexity: sets the flexibility of f'\n",
    "#Example: Maximum tree depth, Minimum samples per leaf, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosing Bias and Variance Problems:\n",
    "\n",
    "# Estimating the Generalization Error:\n",
    "\n",
    "# How do we estimate the generalization error of a model?\n",
    "\n",
    "# Cannot be done directly because:\n",
    "    #f is unknown,\n",
    "    #usually you only have one dataset,\n",
    "    #noise is unpredictable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating the Generalization Error:\n",
    "\n",
    "#Solution:\n",
    "\n",
    " # split the data to training and test sets,\n",
    " # Fit f' to the training set,\n",
    " # evaluate the error of f' on the unseen test set.\n",
    " # generalization error of f' ≈ test set error of f'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better Model Evaluation with Cross-Validation:\n",
    "    \n",
    "# Test set should not be touched until we are condent about f' performance.\n",
    "# Evaluating f' on training set: biased estimate, f' has already seen all training points.\n",
    "# Solution → Cross-Validation (CV):\n",
    "    # K-Fold CV,\n",
    "    # Hold-Out CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diagnose Variance Problems:\n",
    "    \n",
    "#If f' suffers from high variance: CV error of f' > training set error of f'.\n",
    "    #f' is said to overfit the training set. To remedy overtting:\n",
    "        # decrease model complexity,\n",
    "        # for ex: decrease max depth, increase min samples per leaf, ... \n",
    "        # gather more data, ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose Bias Problems:\n",
    "    \n",
    "# if f' suffers from high bias: CV error of ≈training set error of >>desired error.\n",
    "# f' is said to undert the training set. To remedy undertting:\n",
    "    # increase model complexity\n",
    "    # for ex: increase max depth, decrease min samples per leaf, ...\n",
    "    # gather more relevant features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold CV in sklearn on the Auto Dataset:\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set seed for reproducibility:\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = SEED)\n",
    "\n",
    "# Instantiate decision tree regressor and assign it to 'dt':\n",
    "dt = DecisionTreeRegressor(max_depth = 4, min_samples_leaf = 0.14, random_state = SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold CV in sklearn on the Auto Dataset:\n",
    "\n",
    "# Set n_jobs to -1 in order to exploit all CPU cores in computation\n",
    "MSE_CV = - cross_val_score(dt, X_train, y_train, cv = 10, scoring = 'neg_mean_squared_error', n_jobs = -1 )\n",
    "\n",
    "# Fit 'dt' to the training set:  \n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "#Predict the labels of training set:\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "#Predict the labels of test set:\n",
    "y_pred_test = dt.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_error: 18.40848001865519\n",
      "train_error: 14.605635030610792\n",
      "test_error: 19.920062029530225\n"
     ]
    }
   ],
   "source": [
    "CV_error = MSE_CV.mean()\n",
    "print('CV_error:',CV_error)\n",
    "\n",
    "train_error = mean_squared_error(y_pred_train, y_train)\n",
    "print('train_error:',train_error)\n",
    "\n",
    "test_error = mean_squared_error(y_pred_test, y_test)\n",
    "print('test_error:',test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So from the above results, we can see that train_error << test_error. Also CV_error >> train_error.\n",
    "# So our model is overfitted. And its having high variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Learning:\n",
    "\n",
    "# Advantages of CARTs:\n",
    "    \n",
    "# Simple to understand.\n",
    "# Simple to interpret.\n",
    "# Easy to use.\n",
    "# Flexibility: ability to describe non-linear dependencies.\n",
    "# Preprocessing: no need to standardize or normalize features, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limitations of CARTs:\n",
    "    \n",
    "#Classication: can only produce orthogonal decision boundaries.\n",
    "#Sensitive to small variations in the training set.\n",
    "#High variance: unconstrained CARTs may overt the training set.\n",
    "#Solution: ensemble learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Learning:\n",
    "\n",
    "# Train different models on the same dataset.\n",
    "# Let each model make its predictions.\n",
    "# Meta-model: aggregates predictions ofindividual models.\n",
    "# Final prediction: more robust and less prone to errors.\n",
    "# Best results: models are skillful in different ways.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Learning in Practice: Voting Classier:\n",
    "        \n",
    "# Binary classication task.\n",
    "# N classiers make predictions:P ,P , ...,P withP = 0 or 1.\n",
    "# Meta-model prediction: hard voting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classier in sklearn (Breast-Cancer dataset):\n",
    "\n",
    "BC = pd.read_csv('BreastCancer.csv')\n",
    "\n",
    "BC.diagnosis.unique()\n",
    "\n",
    "#M - Maleficent\n",
    "#B - Benign\n",
    "\n",
    "X = BC.drop(['diagnosis', 'id'], axis = 1)\n",
    "\n",
    "y = BC['diagnosis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions to compute accuracy and split data:\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import models, including VotingClassifier meta-model:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Set seed for reproducibility:\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = SEED)\n",
    "\n",
    "# Instantiate individual classifiers:\n",
    "\n",
    "lr = LogisticRegression(random_state = SEED) \n",
    "knn = KNeighborsClassifier()\n",
    "dt = DecisionTreeClassifier(random_state = SEED)\n",
    "\n",
    "# Define a list called classifier that contains the tuples (classifier_name, classifier):\n",
    "\n",
    "classifiers = [ ('LogisticRegression', lr),\n",
    "                ('K Nearest Neighbors', knn),\n",
    "                ('Classification Tree', dt)\n",
    "              ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression  :  0.9532163742690059\n",
      "\n",
      "K Nearest Neighbors  :  0.9298245614035088\n",
      "\n",
      "Classification Tree  :  0.9298245614035088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stayde\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the defined list of tuples containing the classifiers:\n",
    "\n",
    "for clf_name, clf in classifiers:\n",
    "    \n",
    "    # fit clf to the training set:\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels of the test set:\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate the accuracy of clf on the test set:\n",
    "    print()\n",
    "    print(clf_name , ' : ' ,accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On above result, logistic regression will give better accuracy after setting cut off point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier:  0.9590643274853801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stayde\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Voting Classifier in sklearn (Breast-Cancer dataset):\n",
    "\n",
    "# Instantiate a VotingClassifier 'vc':\n",
    "\n",
    "vc = VotingClassifier(estimators = classifiers)\n",
    "\n",
    "# Fit 'vc' to the traing set and predict test set labels:\n",
    "vc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# Evaluate the test-set accuracy of 'vc':\n",
    "\n",
    "print('Voting Classifier: ', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above result, we can see that voting classifier gives better accuracy than each model applied separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging:\n",
    "\n",
    "# Ensemble Methods:\n",
    "\n",
    "# Voting Classier: \n",
    "    # same training set,\n",
    "    # different algorithms.\n",
    "    \n",
    "# Bagging:\n",
    "    # same algorithm,\n",
    "    # different training data sets for each bag (bootstrapped sets of training data).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging:\n",
    "\n",
    "# Bagging: Bootstrap Aggregation.\n",
    "# Uses a technique known as the bootstrap.\n",
    "# Reduces variance of individual models in the ensemble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bagging: Classification & Regression:\n",
    "        \n",
    "#Classication:\n",
    "    #Aggregates predictions by majority voting.\n",
    "    #BaggingClassifier in scikit-learn.\n",
    "\n",
    "#Regression:\n",
    "    #Aggregates predictions through averaging.\n",
    "    #BaggingRegressor in scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging Classifier in sklearn (Breast-Cancer dataset):\n",
    "\n",
    "# Voting Classier in sklearn (Breast-Cancer dataset):\n",
    "import pandas as pd\n",
    "\n",
    "BC = pd.read_csv('BreastCancer.csv')\n",
    "\n",
    "BC.diagnosis.unique()\n",
    "\n",
    "#M - Maleficent\n",
    "#B - Benign\n",
    "\n",
    "X = BC.drop(['diagnosis', 'id'], axis = 1)\n",
    "\n",
    "y = BC['diagnosis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models and utility functions:\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set seed for reproducibility\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a classification-tree 'dt':\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 0.16, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a BaggingClassifier 'bc':\n",
    "\n",
    "bagC = BaggingClassifier(base_estimator = dt, n_estimators = 300, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=4,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=0.16,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        presort=False,\n",
       "                                                        random_state=1,\n",
       "                                                        splitter='best'),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=300, n_jobs=-1, oob_score=False,\n",
       "                  random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit 'bagc' to the training set:\n",
    "\n",
    "bagC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set labels:\n",
    "y_pred = bagC.predict(X_test)\n",
    "\n",
    "# Evaluate and print test-set accuracy:\n",
    "accuracy = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging Classifier: 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Bagging Classifier:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out Of Bag Evaluation:\n",
    "\n",
    "#Bagging:\n",
    "    #Some instances may be sampled severaltimes for one model,\n",
    "    #Other instances may not be sampled at all.\n",
    "\n",
    "\n",
    "#Out Of Bag (OOB) instances:\n",
    "    #On average,for each model, 63% ofthe training instances are sampled.\n",
    "    #The remaining 37% constitute the OOB instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_Score: 0.9239766081871345\n",
      "OOB_Score:  0.9271356783919598\n"
     ]
    }
   ],
   "source": [
    "# OOB Evaluation:\n",
    "\n",
    "# OOB Evaluation in sklearn (Breast Cancer Dataset):\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = SEED)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 0.14, random_state = SEED)\n",
    "\n",
    "bagC = BaggingClassifier(base_estimator = dt, n_estimators = 300, oob_score = True, n_jobs = -1)\n",
    "\n",
    "bagC.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bagC.predict(X_test)\n",
    "\n",
    "print( 'Accuracy_Score:' , accuracy_score(y_pred, y_test))\n",
    "\n",
    "# Extract the OOB accuracy from 'bc':\n",
    "\n",
    "oob_accuracy = bagC.oob_score_\n",
    "\n",
    "print('OOB_Score: ', oob_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests:\n",
    "\n",
    "# Bagging:\n",
    "\n",
    "# Base estimator: Decision Tree, LogisticRegression, Neural Net, ...\n",
    "# Each estimator is trained on a distinct bootstrap sample ofthe training set\n",
    "# Estimators use all features for training and prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Diversity with Random Forests:\n",
    "\n",
    "# Base estimator: Decision Tree\n",
    "# Each estimator is trained on a different bootstrap sample having the same size as the training set\n",
    "# RF introduces further randomization in the training ofindividualtrees\n",
    "# d features are sampled at each node without replacement ( d < total number of features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests: Classication & Regression:\n",
    "\n",
    "# Classication:\n",
    "# Aggregates predictions by majority voting\n",
    "# RandomForestClassifier in scikit-learn\n",
    "\n",
    "# Regression:\n",
    "# Aggregates predictions through averaging\n",
    "# RandomForestRegressor in scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests Regressor in sklearn (auto dataset):\n",
    "\n",
    "#Lets consider auto mpg dataset:\n",
    "\n",
    "auto_old = pd.read_csv('auto.csv')\n",
    "\n",
    "auto = pd.get_dummies(auto_old, drop_first = True)\n",
    "\n",
    "X = auto.drop('mpg', axis =1)\n",
    "\n",
    "y = auto['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9782494582255423"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set seed for reproducibility:\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Instantiate a random forests regressor 'rf' 400 estimators:\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 400, min_samples_leaf = 0.12, random_state = SEED)\n",
    "\n",
    "#Fit 'rf' to the training set:\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels 'y_pred':\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "#Evaluate the test set RMSE:\n",
    "\n",
    "MSE(y_pred, y_test)**(1/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance:\n",
    "\n",
    "# Tree-based methods: enable measuring the importance of each feature in prediction.\n",
    "\n",
    "# In sklearn :\n",
    "    # how much the tree nodes use a particular feature (weighted average) to reduce impurity\n",
    "    # accessed using the attribute feature_importance_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature Importance in sklearn:\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances_rf = pd.Series(rf.feature_importances_, index = X.columns)\n",
    "\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "sorted_importances_rf.plot(kind = 'barh', color = 'lightgreen')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting:\n",
    "\n",
    "# AdaBoost:\n",
    "\n",
    "# Boosting:\n",
    "    \n",
    "#Boosting: Ensemble method combining several weak learners to form a strong learner.\n",
    "#Weak learner: Model doing slightly better than random guessing.\n",
    "#Example of weak learner: Decision stump (CART whose maximum depth is 1)\n",
    "\n",
    "\n",
    "#Boosting:\n",
    "\n",
    "#Train an ensemble of predictors sequentially.\n",
    "#Each predictor tries to correct its predecessor.\n",
    "#Most popular boosting methods:\n",
    "    #AdaBoost,\n",
    "    #Gradient Boosting.\n",
    "\n",
    "\n",
    "#Adaboost:\n",
    "\n",
    "#Stands for Adaptive Boosting.\n",
    "#Each predictor pays more attention to the instances wrongly predicted by its predecessor.\n",
    "#Achieved by changing the weights oftraining instances.\n",
    "#Each predictor is assigned a coefcient α.\n",
    "#α depends on the predictor's training error.\n",
    "\n",
    "#AdaBoost: Training:\n",
    "\n",
    "#Learning Rate: 0 < η ≤ 1\n",
    "\n",
    "# α1 = η * α1\n",
    "\n",
    "# α2 = η * α2\n",
    "\n",
    "# α3 = η * α3\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "# αN = η * αN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost: Prediction:\n",
    "        \n",
    "#Classication:\n",
    "\n",
    "    #Weighted majority voting.\n",
    "    #In sklearn: AdaBoostClassifier.\n",
    "    \n",
    "#Regression:\n",
    "\n",
    "    #Weighted average.\n",
    "    #In sklearn: AdaBoostRegressor .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classication in sklearn (Breast Cancer dataset)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('C:/Users/stayde/Documents/Python Scripts/Python_24_MLwithTreeBasedModels')\n",
    "\n",
    "BC = pd.read_csv('BreastCancer.csv')\n",
    "\n",
    "X = BC.drop('diagnosis', axis = 1)\n",
    "\n",
    "y = BC.diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Set seed for reproducibility:\n",
    "\n",
    "SEED = 1 \n",
    "\n",
    "# Split data into 70% train and 30% test: \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify=y, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "                   base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                         criterion='gini',\n",
       "                                                         max_depth=1,\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         presort=False,\n",
       "                                                         random_state=1,\n",
       "                                                         splitter='best'),\n",
       "                   learning_rate=1.0, n_estimators=300, random_state=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 1, random_state = SEED)\n",
    "\n",
    "adb_clf = AdaBoostClassifier(base_estimator = dt, n_estimators = 300)\n",
    "\n",
    "adb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = adb_clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score:  0.9865654205607477\n"
     ]
    }
   ],
   "source": [
    "print('ROC AUC score: ', adb_clf_roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting (GB):\n",
    "\n",
    "#Gradient Boosted Trees:\n",
    "    \n",
    "#Sequential correction of predecessor's errors.\n",
    "#Does not tweak the weights oftraining instances.\n",
    "#Fit each predictor is trained using its predecessor's residual errors as labels.\n",
    "#Gradient Boosted Trees: a CART is used as a base learner.\n",
    "\n",
    "\n",
    "#Gradient Boosted Trees: Prediction\n",
    "    \n",
    "#Regression:\n",
    "    # y = y + ηr + ... + ηr\n",
    "    # In sklearn: GradientBoostingRegressor \n",
    "    \n",
    "#Classication:\n",
    "    #In sklearn: GradientBoostingClassifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.082222521046935\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting in sklearn (auto dataset):\n",
    "\n",
    "# Lets consider auto mpg dataset:\n",
    "\n",
    "auto_old = pd.read_csv('auto.csv')\n",
    "\n",
    "auto = pd.get_dummies(auto_old, drop_first = True)\n",
    "\n",
    "X = auto.drop('mpg', axis =1)\n",
    "\n",
    "y = auto['mpg']\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=SEED)\n",
    "\n",
    "# Instantiate a GradientBoostingRegressor 'gbt':\n",
    "gb = GradientBoostingRegressor(n_estimators = 300, max_depth = 1, random_state = SEED)\n",
    "\n",
    "# Fit 'gbt' to the training set:\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels:\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE:\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE:\n",
    "print('RMSE:', rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Boosting (SGB):\n",
    "\n",
    "\n",
    "#Gradient Boosting: Cons:\n",
    "        \n",
    "#GB involves an exhaustive search procedure.\n",
    "#Each CART is trained to nd the best split points and features.\n",
    "#May lead to CARTs using the same split points and maybe the same features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic Gradient Boosting:\n",
    "    \n",
    "#Each tree is trained on a random subset of rows ofthe training data.\n",
    "#The sampled instances (40%-80% ofthe training set) are sampled without replacement.\n",
    "#Features are sampled (without replacement) when choosing split points.\n",
    "#Result: further ensemble diversity.\n",
    "#Effect: adding further variance to the ensemble of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 4.18\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Boosting in sklearn (auto dataset):\n",
    "\n",
    "# Import models and utility functions\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=SEED)\n",
    "\n",
    "sgbt = GradientBoostingRegressor(max_depth = 1, subsample = 0.8, max_features = 0.2, n_estimators = 300, random_state = SEED)\n",
    "\n",
    "#Fit sgbt to training set:\n",
    "sgbt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels:\n",
    "y_pred = sgbt.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate test set RMSE:\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print 'rmse_test':\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning a CART's hyperparameters:\n",
    "\n",
    "# Hyperparameters:\n",
    "\n",
    "# Machine learning model:\n",
    "\n",
    "# Difference between Parameters and Hyperparameters:\n",
    "    #parameters: learned from data\n",
    "        #CART example: split-point of a node, split-feature of a node, ...\n",
    "    \n",
    "    #hyperparameters: not learned from data, set prior to training\n",
    "        #CART example: max_depth , min_samples_leaf , splitting criterion ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is hyperparameter tuning?:\n",
    "    \n",
    "# Problem: search for a set of optimal hyperparameters for a learning algorithm.\n",
    "# Solution: find a set of optimal hyperparameters that results in an optimal model.\n",
    "# Optimal model: yields an optimal score.\n",
    "# Score: in sklearn defaults to accuracy (classication) and R (regression).\n",
    "# Cross validation is used to estimate the generalization performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why tune hyperparameters?:\n",
    "    \n",
    "#In sklearn, a model's default hyperparameters are not optimal for all problems.\n",
    "#Hyperparameters should be tuned to obtain the best model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approaches to hyperparameter tuning:\n",
    "    \n",
    "# Grid Search\n",
    "# Random Search\n",
    "# Bayesian Optimization\n",
    "# GeneticAlgorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search cross validation:\n",
    "    \n",
    "# Manually set a grid of discrete hyperparameter values.\n",
    "# Set a metric for scoring model performance.\n",
    "# Search exhaustively through the grid.\n",
    "# For each set of hyperparameters, evaluate each model's CV score.\n",
    "# The optimal hyperparameters are those of the model achieving the best CV score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search cross validation: example:\n",
    "        \n",
    "# Hyperparameters grids:\n",
    "    # max_depth = {2,3,4},\n",
    "    # min_samples_leaf = {0.05, 0.1}\n",
    "# hyperparameter space = { (2,0.05) , (2,0.1) , (3,0.05), ... }\n",
    "# CV scores = { score , ... }\n",
    "# optimal hyperparameters = set of hyperparameters corresponding to the best CV score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the hyperparameters of a CART in sklearn (Breast-Cancer dataset):\n",
    "\n",
    "BC = pd.read_csv('BreastCancer.csv')\n",
    "\n",
    "BC.diagnosis.unique()\n",
    "\n",
    "#M - Maleficent\n",
    "#B - Benign\n",
    "\n",
    "X = BC.drop(['diagnosis', 'id'], axis = 1)\n",
    "\n",
    "y = BC['diagnosis']\n",
    "\n",
    "\n",
    "#Set seed for reproducibility:\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state = SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'presort': False,\n",
       " 'random_state': 1,\n",
       " 'splitter': 'best'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state = SEED)\n",
    "\n",
    "dt.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import GridSearchCV:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Define the grid hyperparameters:\n",
    "\n",
    "params_dt = {\n",
    "                'max_depth' : [3,4,5,6],\n",
    "                'min_samples_leaf' : [0.04, 0.06, 0.08],\n",
    "                'max_features' : [0.2,0.4,0.6,0.8]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a 10-fold CV grid search object 'grid_dt':\n",
    "\n",
    "grid_dt = GridSearchCV( estimator = dt,\n",
    "                        param_grid = params_dt,\n",
    "                        scoring = 'accuracy',\n",
    "                        cv = 10,\n",
    "                        n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort=False, random_state=1,\n",
       "                                              splitter='best'),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_depth': [3, 4, 5, 6],\n",
       "                         'max_features': [0.2, 0.4, 0.6, 0.8],\n",
       "                         'min_samples_leaf': [0.04, 0.06, 0.08]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit 'grid_dt' to the training data:\n",
    "\n",
    "grid_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters are: {'max_depth': 4, 'max_features': 0.2, 'min_samples_leaf': 0.06}\n"
     ]
    }
   ],
   "source": [
    "# Extracting the best hyperparameters from 'grid_dt'\n",
    "\n",
    "best_hyperparams = grid_dt.best_params_\n",
    "\n",
    "print('Best Hyperparameters are:', best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.9346733668341709\n"
     ]
    }
   ],
   "source": [
    "# Extract best CV score from 'grid_dt'\n",
    "\n",
    "best_CV_Score = grid_dt.best_score_\n",
    "\n",
    "print('Best CV accuracy:', best_CV_Score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy of best model: 0.9064327485380117\n"
     ]
    }
   ],
   "source": [
    "# Extracting the best estimator:\n",
    "\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "test_acc = best_model.score(X_test, y_test)\n",
    "\n",
    "print('Test set accuracy of best model:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning an RF's Hyperparameters:\n",
    "\n",
    "# Random Forests Hyperparameters:\n",
    "\n",
    "# CART hyperparameters\n",
    "# number of estimators\n",
    "# bootstrap\n",
    "# ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning is expensive:\n",
    "\n",
    "# Hyperparameter tuning:\n",
    "    #computationally expensive,\n",
    "    #sometimes leads to very slight improvement,\n",
    "\n",
    "# Weight the impact oftuning on the whole project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting RF Hyperparameters in sklearn:\n",
    "\n",
    "#Lets consider auto mpg dataset:\n",
    "\n",
    "auto_old = pd.read_csv('auto.csv')\n",
    "\n",
    "auto = pd.get_dummies(auto_old, drop_first = True)\n",
    "\n",
    "X = auto.drop('mpg', axis =1)\n",
    "\n",
    "y = auto['mpg']\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y,test_size=0.2, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 'warn',\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "rf = RandomForestRegressor(random_state = SEED)\n",
    "\n",
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "SEED = 1 \n",
    "\n",
    "# Define a grid of hyperparameter 'params_rf':\n",
    "\n",
    "params_rf = {\n",
    "    \n",
    "    'n_estimators' : [300, 400, 500],\n",
    "    'max_depth' : [4,6,8],\n",
    "    'max_features' : ['log2', 'sqrt'],\n",
    "    'min_samples_leaf' : [ 0.1, 0.2]            \n",
    "}\n",
    "\n",
    "# Instantiate 'grid_rf':\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "                       estimator = rf,\n",
    "                       param_grid = params_rf,\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        cv = 3,\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:   26.9s finished\n",
      "C:\\Users\\stayde\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestRegressor(bootstrap=True, criterion='mse',\n",
       "                                             max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators='warn', n_jobs=None,\n",
       "                                             oob_score=False, random_state=1,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_depth': [4, 6, 8],\n",
       "                         'max_features': ['log2', 'sqrt'],\n",
       "                         'min_samples_leaf': [0.1, 0.2],\n",
       "                         'n_estimators': [300, 400, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Searching for the best hyperparameters:\n",
    "\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparamters:  {'max_depth': 4, 'max_features': 'log2', 'min_samples_leaf': 0.1, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "#Extracting the best hyperparameters:\n",
    "\n",
    "best_hyperparams = grid_rf.best_params_\n",
    "\n",
    "print('Best hyperparamters: ', best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.525171873474278\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the best model performance:\n",
    "\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict the test set labels:\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the test set RMSE: \n",
    "mse_test = MSE(y_pred, y_test)\n",
    "\n",
    "\n",
    "rmse_test = mse_test ** (1/2)\n",
    "\n",
    "\n",
    "print('RMSE:', rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
